{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to Extract Title, Description, and URL from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import string\n",
    "\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# import sys # import \n",
    "# !{sys.executable} -m pip install polyglot\n",
    "# nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = [\n",
    "        'bloomberg', 'reuters', 'financial-times',\n",
    "        'the-economist', 'the-wall-street-journal', 'cnbc',\n",
    "        'financial-post', 'business-insider', 'australian-financial-review',\n",
    "        'google-news', 'fortune', 'bbc-news',\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 13.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "news_d = {}\n",
    "\n",
    "for i in news:\n",
    "    \n",
    "    \"\"\"\n",
    "    Reads in the API request of the top headlines to clean the JSON data to only return the source, title, and description\n",
    "    \"\"\"\n",
    "    \n",
    "    url = ('https://newsapi.org/v2/top-headlines?'\n",
    "           'sources=' + i + '&'\n",
    "           'apiKey=ddcb36b8f0d645b0acd4a54ef804a9fe')\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    \n",
    "    news_d[i] = []\n",
    "    \n",
    "    for x in range(0, len(response.json()['articles'])):\n",
    "        article_title = response.json()['articles'][x]['title']\n",
    "        article_description = response.json()['articles'][x]['description']\n",
    "        \n",
    "        news_d[i].append({'title': article_title, 'description': article_description})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_d(dictionary):\n",
    "    source_list = [] \n",
    "    title_list = []\n",
    "    description_list = []\n",
    "    \n",
    "    \"\"\"\n",
    "    Takes in the cleaned news dictionary to turn it into an acceptable format for pandas.\n",
    "    1. Check if there are any news articles for that selected news site.\n",
    "    2. Appends the source (e.g. bloomberg) then the article's title and description to the above list\n",
    "    \"\"\"\n",
    "\n",
    "    for i in news_d:\n",
    "        if bool(news_d[i]):\n",
    "            \n",
    "            for x in range(0, len(news_d[i])):\n",
    "                source_list.append(i)\n",
    "                title_list.append(news_d[i][x]['title'])\n",
    "                description_list.append(news_d[i][x]['description'])\n",
    "    \n",
    "    d = {'source': source_list,\n",
    "         'title': title_list,\n",
    "         'description': description_list,\n",
    "        }\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = create_d(news_d)\n",
    "df = pd.DataFrame(data=data)\n",
    "df = df[['source', 'title', 'description']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI Attempt\n",
    "\n",
    "Reference to the following blog:\n",
    "http://blog.chartbeat.com/2015/10/22/identifying-and-clustering-news-events-by-content/\n",
    "\n",
    "And also reference to Jose Portilla's Natural Language Processing course in his Python for Data Science and Machine Learning Bootcamp Course on Udemy:\n",
    "https://www.udemy.com/python-for-data-science-and-machine-learning-bootcamp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(mess):\n",
    "    \"\"\"\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Remove all punctuation\n",
    "    2. Remove all stopwords\n",
    "    3. Returns a list of the cleaned text\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if the cell has characters inside\n",
    "    if mess is not None:\n",
    "\n",
    "        # Check characters to see if they are in punctuation\n",
    "        nohyph = mess.replace('-', ' ')\n",
    "        nopunc = nohyph.replace(\"’s\", ' ')\n",
    "        nopunc = nopunc.replace(\"’\", ' ')\n",
    "        nopunc = nopunc.replace(\"‘\", ' ')\n",
    "        nopunc = [char for char in nopunc if char not in string.punctuation]\n",
    "        \n",
    "\n",
    "\n",
    "        # Join the characters again to form the string.\n",
    "        nopunc = ''.join(nopunc)\n",
    "\n",
    "        # Now just remove any stopwords\n",
    "        word_list = [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]\n",
    "        \n",
    "        return ' '.join(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['details'] = df['title'].astype(str) + ' ' + df['description'].astype(str)\n",
    "# df['details'] = df['title'].astype(str)\n",
    "df['details'] = df['details'].apply(text_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(df['details'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity Matrix\n",
    "\n",
    "Referencing to method in this blog post: https://ematosevic.wordpress.com/2016/08/21/clustering-data-with-similarity-matrix-in-python-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter = 0.2\n",
    "\n",
    "sim = cosine_similarity(X)\n",
    "sim_matrix = pd.DataFrame(sim, columns = list(df['title']), index = list(df['title']))\n",
    "sim_matrix[sim_matrix >= parameter] = 1\n",
    "sim_matrix[sim_matrix < parameter] = 0\n",
    "sim_matrix = sim_matrix.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = []\n",
    "\n",
    "while len(sim_matrix) > 0:\n",
    "    summation = list(sim_matrix.sum(axis=1))\n",
    "    ind = summation.index(max(summation))\n",
    "\n",
    "    titles = list(sim_matrix.columns[(sim_matrix == 1).iloc[ind]])\n",
    "    index_title = sim_matrix['index'][ind]\n",
    "    titles.append(index_title)\n",
    "    titles = list(set(titles))\n",
    "    clusters.append(titles)\n",
    "\n",
    "    sim_matrix = sim_matrix.drop(titles, axis=1)\n",
    "    for title in titles:\n",
    "        sim_matrix = sim_matrix[sim_matrix['index'] != title]\n",
    "\n",
    "    sim_matrix = sim_matrix.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the Justice Department Ready for Prime Time This Time?\n",
      "US appeals decision approving AT&T-Time Warner deal\n",
      "Justice Department to appeal approval of AT&T acquisition of Time Warner\n",
      "Justice Department appeals Time Warner-AT&T merger approval\n",
      "The Justice Department is reportedly going to appeal the AT&T-Time Warner merger\n",
      "Justice Department Appeals Court Ruling Allowing AT&T-Time Warner Merger\n",
      "AT&T Vows to Take Time Warner Case to Supreme Court If Necessary\n",
      "Justice Department to Appeal Antitrust Approval for AT&T-Time Warner Merger\n",
      "=====================\n",
      "A trade war risks all Donald Trump’s economic successes\n",
      "Donald Trump says Theresa May's Brexit plan will 'kill' a US-UK trade deal in explosive interview\n",
      "Trump says Brexit blueprint will kill any UK-US trade deal\n",
      "Trump blasts UK PM May's Brexit plan, says it puts trade deal in doubt\n",
      "Trump: Brexit plan could kill US trade deal\n",
      "Trump Says May’s Brexit Plan Could ‘Kill’ Chances for U.S.-U.K. Trade Deal\n",
      "=====================\n",
      "Trump claims Nato allies have agreed to spending rise\n",
      "Trump’s Complaints About NATO Defense Spending Don’t Add Up\n",
      "Trump ready to help some NATO states buy U.S. arms\n",
      "Trump Reaffirms Commitment to NATO After Strained Emergency Meeting\n",
      "=====================\n",
      "J&J Hit With $550 Million Jury Verdict in Baby Powder Suit\n",
      "Jury orders J&J to pay $4.7 billion in Missouri asbestos cancer case\n",
      "J&J Jury Awards $4.14 Billion Punitive Damages Over Talc Cancer\n",
      "=====================\n",
      "F.B.I. Agent Defends Actions in Russia Inquiry in Contentious House Testimony\n",
      "FBI’s Peter Strzok Breaks Silence on Texts\n",
      "Sparks fly in FBI agent's Trump testimony\n",
      "=====================\n",
      "Commerce Secretary Wilbur Ross Says He’ll Sell All Equity Holdings\n",
      "US Commerce Secretary Wilbur Ross says he will sell all of his stocks, buying Treasurys\n",
      "=====================\n",
      "Exclusive: Jeff Bezos plans to charge at least $200,000 for space rides - sources\n",
      "Jeff Bezos is said to plan to charge at least $200,000 for space rides\n",
      "=====================\n",
      "Stormy Daniels arrested in Ohio strip club, but charges dismissed\n",
      "Charges against Stormy Daniels are dismissed after Ohio strip club arrest\n",
      "=====================\n",
      "With May’s Government Teetering, Trump Gives it a Shove\n",
      "President Trump says Boris Johnson would 'make a great prime minister'\n",
      "=====================\n",
      "House to vote on Democratic bill to abolish ICE\n",
      "Republicans will hold a vote on abolishing ICE — forcing Democrats to show their hand\n",
      "=====================\n",
      "Puerto Rico flag row prompts hate charges\n",
      "Illinois man faces hate crime charges after incident with woman wearing Puerto Rico flag shirt\n",
      "=====================\n",
      "Feds reopen Emmett Till murder case, family 'wants justice to prevail'\n",
      "US reopens Emmett Till murder case\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "for cluster in clusters:\n",
    "    if len(cluster) > 1:\n",
    "        for i in cluster:\n",
    "            print(i)\n",
    "        print('=====================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "df.to_csv('data/{}_input-parameter-('.format(current_time) + str(parameter) + ').csv', sep=',', encoding='utf-8', index=False)\n",
    "\n",
    "clusters_df = pd.DataFrame(clusters, columns = range(1, len(clusters[0])+1))\n",
    "clusters_df.to_csv('data/{}_output-parameter-('.format(current_time) + str(parameter) + ').csv', sep=',', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
